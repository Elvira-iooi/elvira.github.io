<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[MPI的容器化封装]]></title>
    <url>%2F2017%2F11%2F25%2FMPI%E7%9A%84%E5%AE%B9%E5%99%A8%E5%8C%96%E5%B0%81%E8%A3%85%2F</url>
    <content type="text"><![CDATA[MPI的容器化封装技术／基于Docker的HPC平台问题： 1、mpirun hydra； 2、docker封装openmpi、mpich、数学库，若干个版本； 3、docker之间通信网络：通过与主机的bridge；或者，docker之间创建虚拟网络。 ———————————————————————————————————————— 0、知识准备*并行计算基础（六种实现手段）####（一）MPI的认识 MPI：message passing interface，即信息传递接口，包含协议和语义说明。它是一个跨语言的通讯协议，支持点对点和广播，在进程级实现并行计算。（http://mpi-forum.org/） MPI的模型叫做SPMD，single program multiple data，即运行同一个程序，但是处理不同的数据。进程的行为由通讯域（communication world）和该通讯域下的id（rank id）所决定。 *注：MPI可扩展性好，且控制精细度高；但它的编程模型复杂，必须精雕细琢： (1).需要分析及划分应用程序问题，并将问题映射到分布式进程集合； (2).需要解决通信延迟大和负载不平衡的两个主要问题； (3).调试MPI程序麻烦； (4).MPI程序可靠性差，一个进程出问题，整个程序将错误。 实现：OpenMPI：https://www.open-mpi.org/ MPICH：https://www.mpich.org/ 安装及Hellow：移步：https://greenshd.github.io/ （目前只安装了mpich，后续更新openmpi） MPI程序编译环境： * 传统语言（C，C++，Fortran）编译器 + 符合MPI标准的库 MPI程序编译执行的流程是： ===&gt;mpicc -o 编译mpi程序，将mpi.f头文件加载到程序中，形成可执行并行程序； ===&gt;通过mpirun脚本建立并行环境，分配节点/进程rank mpirun：shell脚本，由hydra实现。 hydra：进程管理器（Process Manager，PM）利用ssh、rsh、pbs、slurm和sge等工具部署并运行程序。相对于mpd较为轻量——不用额外启动服务器端，不用进行复杂的服务器配置，只需写一个hosts文件即可。]]></content>
      <categories>
        <category>MPI</category>
      </categories>
      <tags>
        <tag>MPI</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[install mpich]]></title>
    <url>%2F2017%2F11%2F25%2Finstall-mpich%2F</url>
    <content type="text"><![CDATA[实验环境：Ubuntu 16.04 64bit 已安装apt-get、wget、vi/vim、gcc、g++ 三台主机：114.212.82.116 114.212.82.171 114.212.83.36 用户名：apple 密码：****** 0、连网（登bras）命令：curl http://p.nju.edu.cn/portal_io/login -d username=xxxx -d password=xxxxx 1、下载源文件镜像网站，下载比较快：http://www.mpich.org/static/downloads/3.2.1/ 官方网站，下载比较慢：http://www.mpich.org/downloads/ 命令：wget http://www.mpich.org/static/downloads/3.2.1/mpich-3.2.1.tar.gz 2、解压源文件在home/apple下创建安装文件夹：mkdir mpich_install 解压mpich压缩包到该文件夹下：sudo tar -zxvf mpich-3.2.1.tar.gz -C mpich_install 3、编译与安装在home/apple下创建安装路径文件夹：mkdir mpich （注：pwd查看当前路径） 进入mpich_install目录，使用ls命令查看便可发现多出了一个mpich-3.2.1目录 进入mpich-3.2.1目录，进行软件配置与检查，这里只设置安装目录即可。命令： ./configure -prefix=/home/apple/mpich （注：prefix参数表示安装路径，即mpich文件夹） 接下来执行make命令编译，make install命令安装；也可make &amp;&amp; make install 4、配置环境变量编译执行的命令（mpicc、mpirun）是需要添加的，必须添加绝对路径才能正常使用。 修改path，为path添加mpi的bin目录：进入mpich-3.2.1目录 vim ~/.bashrc 在.bashrc文件的末尾添加：export PATH=/home/apple/mpich/bin:$PATH 然后执行 source ~/.bashrc echo $PATH，查看PATH是否发生变化； 注销再登录后，查看bin下的可执行程序：ls /home/apple/mpich/bin 查看命令是否是安装目录下的命令：which mpiexec 5、使用mpi命令编译MPI程序用mpicc，执行MPI程序用mpiexec。 进入mpich-3.2.1目录下，ls可查看到有一个examples文件夹，进行测试确保安装成功： mpirun -np 10 ./examples/cpi （或可以-n） 再进入examples目录下，编译运行hellow： 编译：mpicc -o hellow hellow.c 运行：mpirun -np 4 ./hellow 6、实现各主机间无密码登录如需使用MPI在不同的机器上运行程序，一方面可以使用超级用户权限，另一方面，有些情况下我们不能拥有操作系统的超级用户权限，导致不能改动除用户文件夹以外的文件夹。因此，以下步骤可以使普通用户实现mpi程序的编译和多机器运行。 （0）前提：知道三台主机的ip，用户名相同均为apple，并安装了相同版本号的mpich。 （1）检查各个机器的PATH（该步骤在前几个操作中已经提及，此处不再设置）：mpicc —version 得到如下结果即可继续，否则，返回第4步重新设置PATH： （2）两两之间设置无password登录: 如果当下是机器apple@u194（IP为114.212.82.116），运行以下命令：ssh-keygen -t rsa 不要输入任何内容，直接回车。运行以下命令：ssh apple@114.212.82.171 mkdir -p .ssh 将会提示输入机器apple@u171（IP为114.212.82.171）的password，输入后，敲击回车; 运行以下命令： cat .ssh/id_rsa.pub | ssh apple@114.212.82.171 &apos;cat &gt;&gt; .ssh/authorized_keys’ 然后登录apple@u171：ssh apple@114.212.82.171 继续运行以下命令：ssh-keygen -t rsa 遇到yes/no，yes；其余情况一律回车； 运行以下命令： cat .ssh/id_rsa.pub | ssh apple@114.212.82.116 &apos;cat &gt;&gt; .ssh/authorized_keys’ 最后运行以下命令，以推出机器apple@u171：exit 完成上面的步骤后，就可以实现apple用户在u194和u171之间免密登录了。 其余主机也是如此设置，但需要注意的是，每台主机的rsa密钥只需生成一次，不要重复设置以免覆盖原有的图案。 7、mpi多机器运行（0）首先在apple@u171的/home/apple目录下，新建test.c文件，代码如下： （1）执行命令mpicc -o test test.c 进行编译，生成可执行文件test （2）在apple@u36、apple@u171、apple@u194的/home/apple目录下，分别新建一个hosts文件，用来保存三台机器的IP地址，第一行是本机IP，示例如下：（apple@u171下的hosts内容） （3）然后将apple@u171中，刚刚编译生成的test，复制到其他机器相同目录下，命令为： scp apple@114.212.82.171:/home/apple/test /home/apple （4）在apple@u171的/home/apple目录下，执行命令：mpirun -n 10 -f /home/apple/hosts ./test 得到如下结果： 附：每台机器各个目录下的文件如下所示： 1、apple@u194 （114.212.82.116） 2、apple@u171 （114.212.82.171） 3、apple@u36 （114.212.83.36） u171、u194都有2个process，u36只有一个；如果开6个，则每台主机分到2个process。 所以process的分配是和hosts中写的IP顺序有关吗？]]></content>
  </entry>
</search>
